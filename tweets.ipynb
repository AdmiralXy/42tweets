{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e04cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T16:31:41.957619Z",
     "iopub.status.busy": "2022-01-26T16:31:41.952631Z",
     "iopub.status.idle": "2022-01-26T16:31:44.550177Z",
     "shell.execute_reply": "2022-01-26T16:31:44.550752Z",
     "shell.execute_reply.started": "2022-01-26T16:18:41.642318Z"
    },
    "papermill": {
     "duration": 2.61881,
     "end_time": "2022-01-26T16:31:44.550952",
     "exception": false,
     "start_time": "2022-01-26T16:31:41.932142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lastsign/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lastsign/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lastsign/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lastsign/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import warnings\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from catboost import CatBoostClassifier\n",
    "import pymorphy2\n",
    "from textblob import TextBlob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e9708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               index\n",
       "0              How unhappy  some dogs like it though\n",
       "1  talking to my over driver about where I'm goin...\n",
       "2  Does anybody know if the Rand's likely to fall...\n",
       "3         I miss going to gigs in Liverpool unhappy \n",
       "4      There isnt a new Riverdale tonight ? unhappy "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive = pd.read_csv(\"data/p00_tweets/processedNegative.csv\", squeeze=True).T.reset_index()\n",
    "df_neutral = pd.read_csv(\"data/p00_tweets/processedNeutral.csv\", squeeze=True).T.reset_index()\n",
    "df_negative = pd.read_csv(\"data/p00_tweets/processedPositive.csv\", squeeze=True).T.reset_index()\n",
    "df_positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a874b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T16:31:44.615606Z",
     "iopub.status.busy": "2022-01-26T16:31:44.614755Z",
     "iopub.status.idle": "2022-01-26T16:31:44.617204Z",
     "shell.execute_reply": "2022-01-26T16:31:44.616422Z",
     "shell.execute_reply.started": "2022-01-26T16:18:42.488794Z"
    },
    "papermill": {
     "duration": 0.039175,
     "end_time": "2022-01-26T16:31:44.617433",
     "exception": false,
     "start_time": "2022-01-26T16:31:44.578258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_positive['sentiments'] = 1\n",
    "df_neutral['sentiments'] = 0\n",
    "df_negative['sentiments'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ecd827f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T16:31:44.682492Z",
     "iopub.status.busy": "2022-01-26T16:31:44.68172Z",
     "iopub.status.idle": "2022-01-26T16:31:44.693806Z",
     "shell.execute_reply": "2022-01-26T16:31:44.692905Z",
     "shell.execute_reply.started": "2022-01-26T16:18:42.497457Z"
    },
    "papermill": {
     "duration": 0.048223,
     "end_time": "2022-01-26T16:31:44.693975",
     "exception": false,
     "start_time": "2022-01-26T16:31:44.645752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1570\n",
       "-1    1186\n",
       " 1    1117\n",
       "Name: sentiments, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_negative, df_positive, df_neutral], axis=0, ignore_index=True)\n",
    "df.columns = ['tweets','sentiments']\n",
    "df['sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb8c5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T16:31:44.760606Z",
     "iopub.status.busy": "2022-01-26T16:31:44.759836Z",
     "iopub.status.idle": "2022-01-26T16:31:44.772966Z",
     "shell.execute_reply": "2022-01-26T16:31:44.773815Z",
     "shell.execute_reply.started": "2022-01-26T16:18:42.510057Z"
    },
    "papermill": {
     "duration": 0.050941,
     "end_time": "2022-01-26T16:31:44.773982",
     "exception": false,
     "start_time": "2022-01-26T16:31:44.723041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3873 entries, 0 to 3872\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweets      3873 non-null   object\n",
      " 1   sentiments  3873 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 60.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fadcf2",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a3a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    # remove hyperlinks\n",
    "    document = re.sub(r'http\\S+', '', text.lower())\n",
    "    # remove hashtags symbols\n",
    "    document = re.sub(r'#', '', document)        \n",
    "    ## remove repeating characters\n",
    "    pattern = re.compile(r'(.)\\1{2,}', re.DOTALL)\n",
    "    document = pattern.sub(r\"\\1\\1\", document)\n",
    "    ## remove usernames\n",
    "    document = re.sub(r'@\\S+', '', document)\n",
    "    ## remove all digits and numbers\n",
    "    document = re.sub(r'\\d+', '', document)\n",
    "    # remove special symbols \n",
    "    document = re.sub(r'[^\\w\\s]', '', document)\n",
    "    # replace few spaces to a single one\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce94a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets'] = df['tweets'].apply(remove_symbols)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e9e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1472\n",
       "-1     979\n",
       " 1     975\n",
       "Name: sentiments, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2baa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tweets'], df['sentiments'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd78ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2740,), (686,), (2740,), (686,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f03baf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    return tokens\n",
    "\n",
    "def stemming(tokens):\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def lemmanization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def misspelling(tokens):\n",
    "    return [spell(token) for token in tokens]\n",
    "\n",
    "def stop_words_remove(tokens):\n",
    "    return [token for token in tokens if not token.lower() in stop_words]\n",
    "\n",
    "def preprocessing(tweet, add_mothods):\n",
    "    data = tokenize_tweet(tweet)\n",
    "    for method in add_mothods:\n",
    "        data = method(data)\n",
    "    return data\n",
    "\n",
    "def get_tokenizer(methods):\n",
    "    def wrapper(tweet):\n",
    "        return preprocessing(tweet, methods)\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aec248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_methods = {\n",
    "    'tokenize': [],\n",
    "    'stemming': [stemming],\n",
    "    'lemmanization': [lemmanization],\n",
    "    'stemming_stop_words_remove': [stemming, stop_words_remove],\n",
    "    'misspelling': [misspelling],\n",
    "    'lemmanization_misspelling': [lemmanization, misspelling]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40309fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCV(methods: dict):\n",
    "    best_accuracy_bin_vec = 0\n",
    "    best_accuracy_count_vec = 0\n",
    "    best_accuracy_tfidf_vec = 0\n",
    "    results = {}\n",
    "    for key, methods in methods.items():\n",
    "        \n",
    "        tokenizer_method = get_tokenizer(methods)\n",
    "        \n",
    "        clf = CatBoostClassifier(verbose=False)\n",
    "        \n",
    "        bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer_method(x), binary=True)\n",
    "        count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer_method(x))\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer_method(x))\n",
    "        \n",
    "        grid_pipeline_bin_vec = Pipeline([\n",
    "            ('vectorizer', bin_vectorizer),\n",
    "            ('model', clf)\n",
    "        ])\n",
    "        \n",
    "        grid_pipeline_count_vec = Pipeline([\n",
    "            ('vectorizer', count_vectorizer),\n",
    "            ('model', clf)\n",
    "        ])\n",
    "        \n",
    "        grid_pipeline_tfidf_vec = Pipeline([\n",
    "            ('vectorizer', tfidf_vectorizer),\n",
    "            ('model', clf)\n",
    "        ])\n",
    "        \n",
    "        grid_pipeline_bin_vec.fit(X_train, y_train)\n",
    "        grid_pipeline_count_vec.fit(X_train, y_train)\n",
    "        grid_pipeline_tfidf_vec.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "        y_pred_bin_vec = grid_pipeline_bin_vec.predict(X_test)\n",
    "        y_pred_count_vec = grid_pipeline_count_vec.predict(X_test)\n",
    "        y_pred_tfidf_vec = grid_pipeline_tfidf_vec.predict(X_test)\n",
    "        \n",
    "        accuracy_bin_vec = accuracy_score(y_test, y_pred_bin_vec)\n",
    "        accuracy_count_vec = accuracy_score(y_test, y_pred_count_vec)\n",
    "        accuracy_tfidf_vec = accuracy_score(y_test, y_pred_tfidf_vec)\n",
    "        \n",
    "        if accuracy_bin_vec > best_accuracy_bin_vec:\n",
    "            best_accuracy_bin_vec = accuracy_bin_vec\n",
    "            results['accuracy_bin_vec'] = accuracy_bin_vec\n",
    "            results['methods_names'] = key\n",
    "            results['model_bin_vec'] = grid_pipeline_bin_vec\n",
    "            \n",
    "        if accuracy_count_vec > best_accuracy_count_vec:\n",
    "            best_accuracy_count_vec = accuracy_count_vec\n",
    "            results['accuracy_count_vec'] = accuracy_count_vec\n",
    "            results['methods_names'] = key\n",
    "            results['model_count_vec'] = grid_pipeline_count_vec\n",
    "\n",
    "        if accuracy_tfidf_vec > best_accuracy_tfidf_vec:\n",
    "            best_accuracy_tfidf_vec = accuracy_tfidf_vec\n",
    "            results['accuracy_tfidf_vec'] = accuracy_tfidf_vec\n",
    "            results['methods_names'] = key\n",
    "            results['model_tfidf_vec'] = grid_pipeline_tfidf_vec\n",
    "        print(f'{key} scores {accuracy_bin_vec}; {accuracy_count_vec}; {accuracy_tfidf_vec}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bda9097c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize scores 0.8731778425655977; 0.8731778425655977; 0.8644314868804664\n",
      "stemming scores 0.8615160349854227; 0.8615160349854227; 0.8600583090379009\n",
      "lemmanization scores 0.8688046647230321; 0.8688046647230321; 0.8688046647230321\n",
      "stemming_stop_words_remove scores 0.8513119533527697; 0.8513119533527697; 0.8454810495626822\n",
      "misspelling scores 0.8746355685131195; 0.8746355685131195; 0.8702623906705539\n",
      "lemmanization_misspelling scores 0.8717201166180758; 0.8717201166180758; 0.8615160349854227\n",
      "CPU times: user 20min 19s, sys: 2min 13s, total: 22min 33s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = preprocessCV(preproc_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cef3d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_bin_vec': 0.8746355685131195,\n",
       " 'methods_names': 'misspelling',\n",
       " 'model_bin_vec': Pipeline(steps=[('vectorizer',\n",
       "                  CountVectorizer(binary=True,\n",
       "                                  tokenizer=<function preprocessCV.<locals>.<lambda> at 0x7f5c657dc3a0>)),\n",
       "                 ('model',\n",
       "                  <catboost.core.CatBoostClassifier object at 0x7f5c65728dc0>)]),\n",
       " 'accuracy_count_vec': 0.8746355685131195,\n",
       " 'model_count_vec': Pipeline(steps=[('vectorizer',\n",
       "                  CountVectorizer(tokenizer=<function preprocessCV.<locals>.<lambda> at 0x7f5b2af50af0>)),\n",
       "                 ('model',\n",
       "                  <catboost.core.CatBoostClassifier object at 0x7f5c65728dc0>)]),\n",
       " 'accuracy_tfidf_vec': 0.8702623906705539,\n",
       " 'model_tfidf_vec': Pipeline(steps=[('vectorizer',\n",
       "                  TfidfVectorizer(tokenizer=<function preprocessCV.<locals>.<lambda> at 0x7f5b2af50c10>)),\n",
       "                 ('model',\n",
       "                  <catboost.core.CatBoostClassifier object at 0x7f5c65728dc0>)])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b0dfc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosines 0.9777584952731331 for method tokenize\n",
      "share the love high value members of this week happy insight by\n",
      "share the love high value members of this week happy\n",
      "cosines 0.9783643251697532 for method tokenize\n",
      "thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "cosines 0.9789324358406472 for method tokenize\n",
      "i miss so much unhappy \n",
      "i miss temperance brennan and seeley booth so much unhappy \n",
      "cosines 0.9802677095775874 for method tokenize\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "cosines 0.9834113250021594 for method tokenize\n",
      "thanks for the recent follow much appreciated happy want this\n",
      "thanks for the recent follow much appreciated happy want this for its magical\n",
      "cosines 0.984296760686909 for method tokenize\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9854974266242529 for method tokenize\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9872832330753255 for method tokenize\n",
      " madam and more also in epaper \n",
      " a riot refugees resolve and more also in epaper \n",
      "cosines 0.9944009345274214 for method tokenize\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "cosines 0.9999999999999999 for method tokenize\n",
      " have a great thursday looking forward to reading your tweets happy want this\n",
      " have a great thursday looking forward to reading your tweets happy want this \n",
      "--------------------------------------------------------------------------------------------------\n",
      "cosines 0.9842311678311711 for method stemming\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "cosines 0.9845091852296611 for method stemming\n",
      "share the love thanks for being top new followers this week happy get it\n",
      "share the love thanks for being top new followers this week happy\n",
      "cosines 0.9856090067770884 for method stemming\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9866908930696978 for method stemming\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9870463429062736 for method stemming\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9872832330753255 for method stemming\n",
      " madam and more also in epaper \n",
      " a riot refugees resolve and more also in epaper \n",
      "cosines 0.9876179239443486 for method stemming\n",
      "thanks for the recent follow much appreciated happy want this\n",
      "thanks for the recent follow much appreciated happy want this for its magical\n",
      "cosines 0.9885970205017254 for method stemming\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "cosines 0.9894914947114836 for method stemming\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "cosines 0.9941533703204436 for method stemming\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "--------------------------------------------------------------------------------------------------\n",
      "cosines 0.9783180678039017 for method lemmanization\n",
      "thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "cosines 0.9789324358406472 for method lemmanization\n",
      "i miss so much unhappy \n",
      "i miss temperance brennan and seeley booth so much unhappy \n",
      "cosines 0.979880034237949 for method lemmanization\n",
      "thanks for being top engaged community members this week happy want this its free\n",
      "thanks for being top engaged community members this week happy want this\n",
      "cosines 0.9800842251503105 for method lemmanization\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "cosines 0.9802803151531112 for method lemmanization\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this its free\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this \n",
      "cosines 0.984919747446572 for method lemmanization\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9870264826345184 for method lemmanization\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9883414343258161 for method lemmanization\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "cosines 0.9886861709708371 for method lemmanization\n",
      " madam and more also in epaper \n",
      " a riot refugees resolve and more also in epaper \n",
      "cosines 0.9941051564567942 for method lemmanization\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "--------------------------------------------------------------------------------------------------\n",
      "cosines 0.9745428245288206 for method stemming_stop_words_remove\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "cosines 0.9761221804400126 for method stemming_stop_words_remove\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "cosines 0.9765932528427494 for method stemming_stop_words_remove\n",
      "thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "cosines 0.9815663820836404 for method stemming_stop_words_remove\n",
      "thanks for the recent follow much appreciated happy get it\n",
      "thanks for the recent follow much appreciated happy\n",
      "cosines 0.9845232075045446 for method stemming_stop_words_remove\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "cosines 0.9871125607123649 for method stemming_stop_words_remove\n",
      "share the love thanks for being top new followers this week happy get it\n",
      "share the love thanks for being top new followers this week happy\n",
      "cosines 0.9877195698612901 for method stemming_stop_words_remove\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9882584189124597 for method stemming_stop_words_remove\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9898627245151271 for method stemming_stop_words_remove\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "cosines 0.9999999999999997 for method stemming_stop_words_remove\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosines 0.978367413534302 for method misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "cosines 0.978950756965837 for method misspelling\n",
      "i miss so much unhappy \n",
      "i miss temperance brennan and seeley booth so much unhappy \n",
      "cosines 0.9803339716150034 for method misspelling\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "cosines 0.981725257365042 for method misspelling\n",
      "share the love high value members of this week happy insight by\n",
      "share the love high value members of this week happy\n",
      "cosines 0.9833954950715944 for method misspelling\n",
      "thanks for the recent follow much appreciated happy want this\n",
      "thanks for the recent follow much appreciated happy want this for its magical\n",
      "cosines 0.9843032949147249 for method misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9855547895400274 for method misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.987264245403461 for method misspelling\n",
      " madam and more also in epaper \n",
      " a riot refugees resolve and more also in epaper \n",
      "cosines 0.9943856460766617 for method misspelling\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "cosines 0.9999999999999996 for method misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this \n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "--------------------------------------------------------------------------------------------------\n",
      "cosines 0.9840595719653789 for method lemmanization_misspelling\n",
      "share the love thanks for being top new followers this week happy get it\n",
      "share the love thanks for being top new followers this week happy\n",
      "cosines 0.9849266880957408 for method lemmanization_misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9859780123960487 for method lemmanization_misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9870765194408098 for method lemmanization_misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "cosines 0.9877575086268715 for method lemmanization_misspelling\n",
      "thanks for the recent follow much appreciated happy want this\n",
      "thanks for the recent follow much appreciated happy want this for its magical\n",
      "cosines 0.9883352223843007 for method lemmanization_misspelling\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "cosines 0.98866924205384 for method lemmanization_misspelling\n",
      " madam and more also in epaper \n",
      " a riot refugees resolve and more also in epaper \n",
      "cosines 0.9887573983102647 for method lemmanization_misspelling\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "cosines 0.9940888034578481 for method lemmanization_misspelling\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "cosines 0.9999999999999998 for method lemmanization_misspelling\n",
      "thanks for being top engaged community members this week happy want this\n",
      "thanks for being top engaged community members this week happy want this \n",
      "--------------------------------------------------------------------------------------------------\n",
      "CPU times: user 1min 57s, sys: 807 ms, total: 1min 57s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts_diff_prep = {}\n",
    "for key, methods in preproc_methods.items():\n",
    "        tokenizer_method = get_tokenizer(methods)        \n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer_method(x), ngram_range=(1, 3), max_df=0.5, max_features=1000)\n",
    "        texts_diff_prep[key] = tfidf_vectorizer.fit_transform(df['tweets'])\n",
    "\n",
    "for key in texts_diff_prep.keys():\n",
    "    cosine_sim_tweets = cosine_similarity(texts_diff_prep[key])\n",
    "\n",
    "    cosine_sim_dict = {}\n",
    "    for i in range(len(cosine_sim_tweets)):\n",
    "        for j in range(len(cosine_sim_tweets)):\n",
    "            if i > j:\n",
    "                if cosine_sim_tweets[i][j] < 1. and cosine_sim_tweets[i][j] != 0.:\n",
    "                    cosine_sim_dict[f'{cosine_sim_tweets[i][j]}'] = (i, j)\n",
    "                    \n",
    "    cosine_sim_val = list(cosine_sim_dict.keys())\n",
    "    sorted_cosine_sim_val = np.sort(cosine_sim_val, kind='mergesort')\n",
    "    \n",
    "    for i, val in enumerate(sorted_cosine_sim_val[-10:]):\n",
    "        print(f'cosines {val} for method {key}')\n",
    "\n",
    "        left_cosine = df['tweets'].iloc[cosine_sim_dict[val][0]]\n",
    "        right_cosine = df['tweets'].iloc[cosine_sim_dict[val][1]]\n",
    "        print(f'{left_cosine}\\n{right_cosine}')\n",
    "    \n",
    "    print('--------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc600a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "embeddings_pretrained = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c540b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8940135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_nltk(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tknzr.tokenize(text.lower()) if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0607ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_words = [preproc_nltk(text).split() for text in df['tweets']]\n",
    "embeddings_trained = Word2Vec(proc_words, # data for model to train on\n",
    "                 vector_size=100,                 # embedding vector size\n",
    "                 min_count=3,             # consider words that occured at least 5 times\n",
    "                 window=3).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a35190d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sum(comment, embeddings):\n",
    "    \"\"\"\n",
    "    implement a function that converts preprocessed comment to a sum of token vectors\n",
    "    \"\"\"\n",
    "    embedding_dim = embeddings.vectors.shape[1]\n",
    "    features = np.zeros([embedding_dim], dtype='float32')\n",
    "\n",
    "    for word in preproc_nltk(comment).split():\n",
    "        if word in embeddings:\n",
    "            features += embeddings[f'{word}']\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6812fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_trained.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce643113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2740, 25), (686, 25))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_pretrained) for text in df['tweets']])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, df['sentiments'], test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49723375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce16db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8250728862973761"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=5000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ab4a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2740, 100), (686, 100))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in df['tweets']])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, df['sentiments'], test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7193b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "872f85d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7842565597667639"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33d4bccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8279883381924198"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = CatBoostClassifier(verbose=False)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e254971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8061224489795918"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC()\n",
    "wv_model = svc.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abece7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017492711370262"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "wv_model = sgd.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75e854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 124.45463,
   "end_time": "2022-01-26T16:33:38.253552",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-26T16:31:33.798922",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
